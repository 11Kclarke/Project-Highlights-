# Project-Highlights-
Some Images and Descriptions of Past Projects. Extension of my CV

## Automatic Key Point Value Data Validation 

A data validation tool designed to identify extreme outliers from a vast set of aviation statistics. Tens of Thousands of kinds of values, e.g. 'Max Groundspeed During Takeoff', with thousands of different values created per flight. Sometimes incorrect due to broken sensors, faulty code, etc. As the tool needs to deal with an extremely diverse set of values, I took a very generalist approach and leveraged conditional distributions and Bayesian analysis heavily. As not all of these values are expected to be normally distributed, and I didn't want to write code for every possible distribution, I also made heavy use of sklearn's [Quantile transformer](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_map_data_to_normal.html).
This project, when combined with a similar event validation project I maintain (not discussed here), saves 100s of hours of analyst time a week that would otherwise need to be spent manually validating events.
The tool works by comparing a value being validated to various different conditional distributions of other instances of the same kind of value. One for the aircraft model, one for the analysis specification and version number, etc. For the first run value type, the tool accumulates values until each distribution has a sufficient population. A bootstrapping process of iteratively repeating the validation process while recalculating variance and mean for the next iteration can begin. For values not already normally distributed the mapping to normal is also refit each bootstrap iteration. This reduces the chance of inncorect values affecting the mean and variance used to identify anomalies. Once the cleaned distributions have been formed, only the count, mean, and variance of the normal mapping, and the fitted normal transform are saved. The mean and variance shouldnt change much but are updated using running formula afer each run. 
Even a quantile transformer cannot always create an isomorphism to a normal distribution without horrifically overfitting. I found multimodal distributions where each mode is non-symmetric, particularly troublesome. I managed to mitigate these issues by identifying problematic distributions pre-transformation and writing custom solutions to the common low-hanging fruit.
For example, a distribution resembling 2 mirrored Gumbel distributions formed from taking the most extreme value from a set of samples of normally distributed values, repeatedly. Can be separated into 2 separate distributions before 1 side is flipped and mapped onto the other, such that the means are overlapping. This forms a close enough to normal distribution that a quantile transformer can do the rest. Some values, such as 'Latitude at Touchdown', can never be meaningfully mapped to a normal distribution, but the combination of custom solutions and the Quantile transformer was sufficient for the vast majority of not already normally distributed values.
One issue is that a truly normally distributed value with a large portion in error but wrong in the same way, may appear bimodal. While maintaining blindness to the context of the values, this can't be properly solved beyond statistical power based on sample size.
Many less extreme but still inaccurate data points end up being consecutive, this happens as a result of broken sensors not being fixed for a few weeks or whatever else caused the issue, persisting. Leveraging this, I flagged a significant number of incorrect values. 10 samples in a row, all with Z score > 2, is an extremely strong indicator that something different was happening than the rest of the distribution.

## Fuel Efficiency Models and Dashboard

One of my first ever projects was analysing fuel burn in commercial aviation, looking for potential savings. This resulted in 14 fuel efficiency metrics revealing where and how much fuel could be saved.
This project also resulted in a set of time series LightGBM regression models (1 per aircraft model) that used flight recorder data to estimate fuel burn. When compared to recorded fuel burn, my models significantly outperformed [alternatives](https://openap.dev/fuel_emission.html). However, I did have access to much higher-quality training data. These models allowed us to evaluate counterfactuals, such as fuel burn in flight profiles we believed superior, by modifying flight recorder data before the model ingests the data. As an example, a decent profile would be analysed and a more optimal profile generated. This flight data is then modified to match the optimised profile, and a fuel burn model is used to evaluate the optimised fuel burn. By the close of the project, potential savings were in the 100s of kg per flight. This is proportionally small but adds up. Admittedly, I don't think airlines have yet seen savings that large due to limitations on any changes to flight behaviour. This project also included a Streamlit dashboard used to view and aggregate results.
 Due to flight data being quite heavy and the requirement to display aggregations of results from large sets of flights, a significant amount of thought needed to be put into data engineering. I ended up making my own caching system that would attempt to retrieve a value from an SQLite DB and, only upon failing, generate the value before saving. This also extended to aggregations and required all aggregations to be parameterised by the group the aggregation is based.

## Study on Change In Turbulence Frequency 

Validated [this meteorological study](https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2023GL103814) on the effect of global warming on turbulence rates by leveraging Uber's H3 geospatial indexing and massive amounts of past turbulence event information. The visualisation and validation of results generated significant interest from airlines at aviation conferences. For simplicity, I only looked at whether a 'severe' grade of turbulence was recorded. I think this and my comparison, not going back as far, are responsible for the significant changes in the magnitude of results. However, despite the magnitudes of effect being different, the same areas were all highlighted as increasing with similar relative rates. It's worth noting that the original study uses mathematical meteorological modelling to predict where turbulence will increase. I simply took flight recorder data and, after cleaning, found the rates of turbulence being reported in a hex adjusted for the amount of time aircraft spent in said hex. In the choropleth below, only cells that had at least 40K flights pass through are shaded.

<img src="https://github.com/11Kclarke/Project-Highlights-/blob/main/turbulence.png" alt="Visualization of Local Turbulence Changes"/>



## Generative Art

Generative are was one of the first things that got me interested in programming and maths. I sadly haven't had much time to work on any of it for a while, but you can find an infinite-depth GPU-accelerated fractal zoomer on my GitHub. As it uses OpenCL, it should be runnable even with an AMD GPU without much fuss. It also allows for defining your own customer fractal seed equations instead of just the normal mandlebrott X^2+c. The zoomer isn't limited to escape-type fractals and extends the use of customer seeds equations to Julia Set, like fractals and more. There's also code for plotting images resulting from the distance of jumps during attractor iteration, which produces images that I so far haven't seen anything similar to. It's obscure and useless enough, it might even be unique.

Example Attractor Art      |  Zoom of Inner Structure of bulbs 
:-------------------------:|:-------------------------:
![Example Attractor Art](https://github.com/11Kclarke/Project-Highlights-/blob/main/unknown.png) |  ![Inner Structure of Bulbs](https://github.com/11Kclarke/Project-Highlights-/blob/main/hoppalong-range-xy-are-x0y0-80-80%20zoomed%20spiral.png)


  The most impressive component is probably the mapper from Sympy to Opencl syntax complete for any complex function writeable in expoential form Ie pretty much everything. This allows for GPU based iterations of custom functions. In retrospect im suprised this works given my experience level when writing it.

